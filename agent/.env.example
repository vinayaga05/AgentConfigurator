# Model Provider Configuration
# Options: "gemini" or "ollama"
MODEL_PROVIDER=ollama

# Gemini Model Configuration
# For Gemini: "gemini-2.5-flash", "gemini-2.0-flash", etc.
# Note: Requires GOOGLE_API_KEY to be set (see below)
MODEL_NAME=gemini-2.5-flash

# Ollama Model Configuration
# For Ollama: "llama3.2", "mistral", "qwen2.5", "deepseek-r1:8b", etc.
# Make sure the model is pulled first: ollama pull <model_name>
# To list available models: ollama list
OLLAMA_MODEL=deepseek-r1:8b
OLLAMA_BASE_URL=http://localhost:11434

# Model Temperature Configuration
# Temperature for chat/conversation (default: 0.7)
# Higher values make output more creative, lower values more deterministic
MODEL_TEMPERATURE=0.7

# Temperature for analysis tasks (default: 0.3)
# Lower temperature for more deterministic analysis results
ANALYSIS_TEMPERATURE=0.3

# Gemini API Key (only needed if MODEL_PROVIDER=gemini)
# Get your API key from: https://aistudio.google.com/app/apikey
# Uncomment and set your API key:
GOOGLE_API_KEY=AIzaSyBhNUo1gNHVEheZVpsE5Y3DFl2CKrLxWao
